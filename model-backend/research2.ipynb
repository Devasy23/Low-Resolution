{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Add, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "# Define the generator network\n",
    "def build_generator():\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, None, 3))\n",
    "    \n",
    "    # First convolutional block\n",
    "    x = Conv2D(64, kernel_size=9, padding='same')(input_layer)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    for i in range(16):\n",
    "        residual = x\n",
    "        x = Conv2D(64, kernel_size=3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(64, kernel_size=3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([x, residual])\n",
    "    \n",
    "    # Second convolutional block\n",
    "    x = Conv2D(64, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, input_layer])\n",
    "    \n",
    "    # Upsampling blocks\n",
    "    x = UpSampling2D(size=2)(x)\n",
    "    x = Conv2D(256, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D(size=2)(x)\n",
    "    x = Conv2D(256, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(3, kernel_size=9, padding='same')(x)\n",
    "    output_layer = Activation('tanh')(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the discriminator network\n",
    "def build_discriminator():\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, None, 3))\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding='same')(input_layer)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = Dense(1024)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the SRGAN model\n",
    "def build_srgan(generator, discriminator):\n",
    "    # Freeze the discriminator weights\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, None, 3))\n",
    "    \n",
    "    # Generate high-resolution image\n",
    "    generated_image = generator(input_layer)\n",
    "    \n",
    "    # Extract features from generated image\n",
    "    features = VGG19(include_top=False, weights='imagenet')(generated_image)\n",
    "    \n",
    "    # Define the combined model\n",
    "    model = Model(inputs=input_layer, outputs=[generated_image, discriminator(generated_image), features])\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss=['mse', 'binary_crossentropy', 'mse'], loss_weights=[1, 1e-3, 1], optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "X = []\n",
    "y = []\n",
    "path = 'Data/'\n",
    "for folder in os.listdir(path):\n",
    "  # if folder is a directory\n",
    "  if os.path.isdir(path+folder):\n",
    "    for file in os.listdir(path+folder):\n",
    "      img = cv2.imread(path+folder+'/'+file)\n",
    "      img = cv2.resize(img,(96, 96))\n",
    "      X.append(img)\n",
    "      y.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((414, 96, 96, 3), (414,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000150A0B0A560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 13s 13s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_16\" is incompatible with the layer: expected shape=(None, 96, 96, 3), found shape=(16, 384, 384, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Owner\\Documents\\GitHub\\Low-Resolution\\model-backend\\research2.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/Documents/GitHub/Low-Resolution/model-backend/research2.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m fake_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((batch_size, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/Documents/GitHub/Low-Resolution/model-backend/research2.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m discriminator_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch(high_res_images, real_labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Owner/Documents/GitHub/Low-Resolution/model-backend/research2.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m discriminator_loss_fake \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39;49mtrain_on_batch(fake_high_res_images, fake_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/Documents/GitHub/Low-Resolution/model-backend/research2.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m discriminator_loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39madd(discriminator_loss_real, discriminator_loss_fake)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/Documents/GitHub/Low-Resolution/model-backend/research2.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Train the generator network\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2383\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileywbzr1hl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1146\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1143\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[0;32m   1145\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1146\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1147\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1148\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m )\n\u001b[0;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1135\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1135\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1136\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:993\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m--> 993\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    994\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_16\" is incompatible with the layer: expected shape=(None, 96, 96, 3), found shape=(16, 384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "from model.srgan import generator, discriminator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "# ...\n",
    "\n",
    "# Define the generator and discriminator networks\n",
    "generator = generator()\n",
    "discriminator = discriminator()\n",
    "\n",
    "# Compile the discriminator model\n",
    "optimizer = Adam(lr=1e-4)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# Define the SRGAN model\n",
    "srgan = build_srgan(generator, discriminator)\n",
    "\n",
    "# Compile the SRGAN model\n",
    "optimizer = Adam(lr=1e-4)\n",
    "srgan.compile(loss=['mse', 'binary_crossentropy', 'binary_crossentropy'], loss_weights=[1, 1e-3, 1e-3], optimizer=optimizer)\n",
    "\n",
    "# Train the SRGAN model\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X_train) // batch_size):\n",
    "        # Generate a batch of low-resolution images\n",
    "        low_res_images = []\n",
    "        for img in X_train[i * batch_size:(i + 1) * batch_size]:\n",
    "            img_resized = cv2.resize(img, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "            low_res_images.append(img_resized)\n",
    "        low_res_images = np.array(low_res_images)\n",
    "\n",
    "        # Generate a batch of high-resolution images\n",
    "        high_res_images = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        # Generate a batch of fake high-resolution images\n",
    "        fake_high_res_images = generator.predict(low_res_images)\n",
    "\n",
    "        # Train the discriminator network\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "        discriminator_loss_real = discriminator.train_on_batch(high_res_images, real_labels)\n",
    "        discriminator_loss_fake = discriminator.train_on_batch(fake_high_res_images, fake_labels)\n",
    "        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "        # Train the generator network\n",
    "        perceptual_loss, content_loss, adversarial_loss = srgan.train_on_batch(low_res_images, [high_res_images, real_labels, real_labels])\n",
    "\n",
    "        # Print the loss\n",
    "        print('Epoch %d, Batch %d: Discriminator Loss = %s, Adversarial Loss = %s, Content Loss = %s, Perceptual Loss = %s' % (epoch, i, discriminator_loss, adversarial_loss, content_loss, perceptual_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
